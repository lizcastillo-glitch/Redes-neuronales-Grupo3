#  Red Neuronal Feedforward desde Cero (NumPy) - Grupo 3

![Python](https://img.shields.io/badge/Python-3.8%2B-blue)
![NumPy](https://img.shields.io/badge/Library-NumPy-orange)
![Status](https://img.shields.io/badge/Status-Academic%20Project-green)

Implementaci贸n completa de una **Red Neuronal Artificial (Multi-Layer Perceptron)** construida puramente con **NumPy**, sin utilizar frameworks de Deep Learning (como TensorFlow o PyTorch) para el c谩lculo de gradientes.

El objetivo principal es desmitificar la "caja negra" del aprendizaje profundo, implementando manualmente el **Forward Propagation**, **Backpropagation** y el **Descenso de Gradiente**.

##  Tabla de Contenidos
- [Descripci贸n del Proyecto](#descripci贸n-del-proyecto)
- [Estructura del Repositorio](#estructura-del-repositorio)
- [Caracter铆sticas T茅cnicas](#caracter铆sticas-t茅cnicas)
- [Instalaci贸n y Uso](#instalaci贸n-y-uso)
- [Metodolog铆a](#metodolog铆a)
- [Resultados y Comparativa](#resultados-y-comparativa)
- [Autores](#autores)

##  Descripci贸n del Proyecto
Este proyecto aborda un problema de **Clasificaci贸n de Sentimientos** (Positivo, Negativo, Neutro) utilizando un pipeline completo de Machine Learning:
1.  **Preprocesamiento:** Limpieza de texto y Aumentaci贸n de Datos (Data Augmentation).
2.  **Vectorizaci贸n:** TF-IDF (Term Frequency - Inverse Document Frequency).
3.  **Modelado:** Red Neuronal Feedforward con arquitectura din谩mica construida desde cero.
4.  **Optimizaci贸n:** Backpropagation manual con optimizador SGD (Stochastic Gradient Descent) por mini-batches.

##  Estructura del Repositorio

## 锔 Caracter铆sticas T茅cnicas
La clase `NeuralNetwork` implementada en `src/neural_network.py` soporta:
* **Arquitectura Din谩mica:** Definici贸n arbitraria de capas ocultas (ej. `[Input, 128, 64, Output]`).
* **Funciones de Activaci贸n:**
    * `ReLU` (optimizada con inicializaci贸n **He** para evitar *vanishing gradients*).
    * `Tanh` y `Sigmoid` (con inicializaci贸n **Xavier/Glorot**).
    * `Softmax` (para la capa de salida multi-clase).
* **Optimizador:** Mini-Batch Gradient Descent.
* **Funci贸n de Costo:** Cross-Entropy Loss (con estabilidad num茅rica).

##  Instalaci贸n y Uso

1.  **Clonar el repositorio:**
    ```bash
    git clone [https://github.com/lizcastillo-glitch/Redes-neuronales-Grupo3.git](https://github.com/lizcastillo-glitch/Redes-neuronales-Grupo3.git)
    cd Redes-neuronales-Grupo3
    ```

2.  **Instalar dependencias:**
    ```bash
    pip install numpy pandas scikit-learn matplotlib seaborn
    ```

3.  **Ejecuci贸n:**
    Se recomienda abrir los notebooks en Google Colab o Jupyter Lab siguiendo el orden num茅rico para replicar el proceso desde la construcci贸n de la clase hasta la experimentaci贸n.

## И Metodolog铆a
Para validar el modelo, se realizaron experimentos comparativos variando:
* **Arquitecturas:** Profundidad (2+ capas ocultas) y ancho de capas.
* **Hiperpar谩metros:** Learning Rate (0.05, 0.01, 0.005) y Epochs.
* **Data Augmentation:** Se implement贸 una t茅cnica de generaci贸n de texto sint茅tico para robustecer el dataset original peque帽o.
* **Baseline:** Se compar贸 el rendimiento contra una **Regresi贸n Log铆stica** de Scikit-Learn.

##  Resultados y Comparativa

Se encontr贸 que la combinaci贸n de **ReLU + Inicializaci贸n He** ofreci贸 la convergencia m谩s r谩pida y estable. A continuaci贸n, un resumen de los hallazgos clave:

| Modelo | Activaci贸n | Init | F1-Score | Observaci贸n |
| :--- | :--- | :--- | :--- | :--- |
| **Red Neuronal (Propia)** | **ReLU** | **He** | **1.00** | Mejor rendimiento y convergencia r谩pida. |
| Red Neuronal (Propia) | Tanh | Xavier | 1.00 | Buen rendimiento, ligeramente m谩s lenta en converger. |
| Red Neuronal (Propia) | Sigmoid | Xavier | 0.16 - 0.72 | Sufri贸 severamente de *Vanishing Gradient*. |
| Baseline (LogReg) | - | - | 1.00 | Modelo lineal simple, efectivo para este dataset separable. |

> **Nota:** El Accuracy perfecto (1.0) se debe a la naturaleza sint茅tica y altamente separable del dataset de prueba. En un entorno de producci贸n real con datos ruidosos, se esperar铆an m茅tricas m谩s variadas.

### Visualizaciones
Las curvas de aprendizaje y matrices de confusi贸n generadas durante los experimentos se encuentran almacenadas en la carpeta `/results`.

##  Autores
**Grupo 3**

* Liz Eliana Castillo Zamora

* Pablo Mauricio Castro Hinostroza

* Erick Sebasti谩n Rivas

* ngel Israel Romero Medina

---
*Este proyecto fue desarrollado con fines acad茅micos.*
