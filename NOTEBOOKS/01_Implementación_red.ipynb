{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##NOTEBOOK 1 ##\n",
        "IMPLEMENTACION DE UNA RED NEURONAL FEEDFORWARD DESDE CERO (NumPy)\n",
        "\n",
        "El objetivo es construir una red con 2+ capas ocultas, activaciones Sigmoid/ReLU/Tanh, inicialización Xavier/He, forward, backward y entrenamiento con gradiente descendente.\n",
        "\n",
        "El notebook es reutilizable para el pipeline de clasificación de sentimientos del Proyecto 2 (se conectará en el Notebook 02 con TF-IDF)"
      ],
      "metadata": {
        "id": "5ADSSRnFVmxL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Imports y utilidades"
      ],
      "metadata": {
        "id": "2x9Hnm_aWGqT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPLbSKwLVgNw"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Activaciones y derivadas"
      ],
      "metadata": {
        "id": "gJ12hVqlWSPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(z): return 1/(1+np.exp(-z))\n",
        "def dsigmoid(a): return a*(1-a)\n",
        "\n",
        "def tanh(z): return np.tanh(z)\n",
        "def dtanh(a): return 1 - a**2\n",
        "\n",
        "def relu(z): return np.maximum(0, z)\n",
        "def drelu(z): return (z > 0).astype(float)\n",
        "\n",
        "def softmax(z):\n",
        "    z = z - np.max(z, axis=1, keepdims=True)\n",
        "    exp = np.exp(z)\n",
        "    return exp / np.sum(exp, axis=1, keepdims=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "zbQblFadWUdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Inicialización Xavier/He"
      ],
      "metadata": {
        "id": "TO8SZDmiWZca"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(fan_in, fan_out, init=\"he\"):\n",
        "    if init == \"he\":\n",
        "        scale = np.sqrt(2.0 / fan_in)\n",
        "    else:  # xavier\n",
        "        scale = np.sqrt(1.0 / fan_in)\n",
        "    return np.random.randn(fan_in, fan_out) * scale\n"
      ],
      "metadata": {
        "id": "-wgfUvrNWfgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Clase NeuralNetwork (2+ capas ocultas)"
      ],
      "metadata": {
        "id": "lxF41BcXWhX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NeuralNetwork:\n",
        "    def __init__(self, layers, activation=\"relu\", init=\"he\", seed=42):\n",
        "        \"\"\"\n",
        "        layers: lista ej. [n_in, 128, 64, n_out]\n",
        "        activation: \"relu\" | \"tanh\" | \"sigmoid\"\n",
        "        init: \"he\" | \"xavier\"\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        self.layers = layers\n",
        "        self.activation_name = activation\n",
        "        self.init = init\n",
        "\n",
        "        self.W = []\n",
        "        self.b = []\n",
        "        for i in range(len(layers)-1):\n",
        "            self.W.append(init_weights(layers[i], layers[i+1], init=init))\n",
        "            self.b.append(np.zeros((1, layers[i+1])))\n",
        "\n",
        "    def _act(self, z):\n",
        "        if self.activation_name == \"relu\": return relu(z)\n",
        "        if self.activation_name == \"tanh\": return tanh(z)\n",
        "        return sigmoid(z)\n",
        "\n",
        "    def _dact(self, z, a):\n",
        "        if self.activation_name == \"relu\": return drelu(z)\n",
        "        if self.activation_name == \"tanh\": return dtanh(a)\n",
        "        return dsigmoid(a)\n",
        "\n",
        "    def forward(self, X):\n",
        "        A = X\n",
        "        self.cache = {\"A\": [X], \"Z\": []}\n",
        "        # capas ocultas\n",
        "        for i in range(len(self.W)-1):\n",
        "            Z = A @ self.W[i] + self.b[i]\n",
        "            A = self._act(Z)\n",
        "            self.cache[\"Z\"].append(Z)\n",
        "            self.cache[\"A\"].append(A)\n",
        "        # salida (softmax)\n",
        "        ZL = A @ self.W[-1] + self.b[-1]\n",
        "        AL = softmax(ZL)\n",
        "        self.cache[\"Z\"].append(ZL)\n",
        "        self.cache[\"A\"].append(AL)\n",
        "        return AL\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        # y_true one-hot\n",
        "        eps = 1e-9\n",
        "        return -np.mean(np.sum(y_true * np.log(y_pred + eps), axis=1))\n",
        "\n",
        "    def backward(self, X, y_true):\n",
        "        \"\"\"\n",
        "        Backprop para softmax + cross-entropy:\n",
        "        dZL = (y_pred - y_true)/m\n",
        "        \"\"\"\n",
        "        m = X.shape[0]\n",
        "        A_list = self.cache[\"A\"]\n",
        "        Z_list = self.cache[\"Z\"]\n",
        "        y_pred = A_list[-1]\n",
        "\n",
        "        dW = [None]*len(self.W)\n",
        "        db = [None]*len(self.b)\n",
        "\n",
        "        dZ = (y_pred - y_true) / m  # salida\n",
        "\n",
        "        # última capa\n",
        "        A_prev = A_list[-2]\n",
        "        dW[-1] = A_prev.T @ dZ\n",
        "        db[-1] = np.sum(dZ, axis=0, keepdims=True)\n",
        "\n",
        "        # capas ocultas hacia atrás\n",
        "        dA_prev = dZ @ self.W[-1].T\n",
        "\n",
        "        for i in reversed(range(len(self.W)-1)):\n",
        "            Z = Z_list[i]\n",
        "            A = A_list[i+1]\n",
        "            dZ = dA_prev * self._dact(Z, A)\n",
        "            A_prev = A_list[i]\n",
        "            dW[i] = A_prev.T @ dZ\n",
        "            db[i] = np.sum(dZ, axis=0, keepdims=True)\n",
        "            if i != 0:\n",
        "                dA_prev = dZ @ self.W[i].T\n",
        "\n",
        "        return dW, db\n",
        "\n",
        "    def train(self, X, y, epochs=20, lr=0.01, batch_size=32, verbose=True):\n",
        "        # one-hot\n",
        "        n_classes = len(np.unique(y))\n",
        "        y_onehot = np.eye(n_classes)[y]\n",
        "\n",
        "        history = {\"loss\": [], \"acc\": []}\n",
        "        for epoch in range(1, epochs+1):\n",
        "            # shuffle\n",
        "            idx = np.random.permutation(len(X))\n",
        "            Xs, ys = X[idx], y_onehot[idx]\n",
        "            y_labels = y[idx]\n",
        "\n",
        "            t0 = time.time()\n",
        "            for i in range(0, len(Xs), batch_size):\n",
        "                Xb = Xs[i:i+batch_size]\n",
        "                yb = ys[i:i+batch_size]\n",
        "                yb_labels = y_labels[i:i+batch_size]\n",
        "\n",
        "                y_pred = self.forward(Xb)\n",
        "                dW, db = self.backward(Xb, yb)\n",
        "\n",
        "                # update\n",
        "                for k in range(len(self.W)):\n",
        "                    self.W[k] -= lr * dW[k]\n",
        "                    self.b[k] -= lr * db[k]\n",
        "\n",
        "            # métricas\n",
        "            y_pred_full = self.forward(X)\n",
        "            loss = self.compute_loss(y_onehot, y_pred_full)\n",
        "            acc = np.mean(np.argmax(y_pred_full, axis=1) == y)\n",
        "\n",
        "            history[\"loss\"].append(loss)\n",
        "            history[\"acc\"].append(acc)\n",
        "\n",
        "            if verbose and (epoch == 1 or epoch % 5 == 0):\n",
        "                print(f\"Epoch {epoch:02d} | loss={loss:.4f} | acc={acc:.4f} | {time.time()-t0:.1f}s\")\n",
        "        return history\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.forward(X)\n",
        "        return np.argmax(probs, axis=1), probs\n"
      ],
      "metadata": {
        "id": "X3d51xKyWmnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Mini prueba rápida"
      ],
      "metadata": {
        "id": "8cPY3_qkWtU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Datos sintéticos: 3 clases\n",
        "np.random.seed(0)\n",
        "X_demo = np.random.randn(300, 20)\n",
        "y_demo = np.random.choice([0,1,2], size=300)\n",
        "\n",
        "nn = NeuralNetwork(layers=[20, 64, 32, 3], activation=\"relu\", init=\"he\")\n",
        "hist = nn.train(X_demo, y_demo, epochs=10, lr=0.01, batch_size=32)\n"
      ],
      "metadata": {
        "id": "ZyywrmOMWxid",
        "outputId": "27c00b73-0901-4694-bbf4-cc1db22c8fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 01 | loss=1.3654 | acc=0.3333 | 0.0s\n",
            "Epoch 05 | loss=1.1918 | acc=0.3533 | 0.0s\n",
            "Epoch 10 | loss=1.0990 | acc=0.4200 | 0.0s\n"
          ]
        }
      ]
    }
  ]
}